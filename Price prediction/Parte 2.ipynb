{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Dropout #type: ignore\n",
    "from tensorflow.keras.optimizers import Adam #type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical #type: ignore\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50 #type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input #type: ignore\n",
    "from sklearn.metrics import confusion_matrix #type: ignore\n",
    "import tensorflow.keras.backend as K #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = tfds.load(\"caltech101\", with_info=True, as_supervised=True) #Carico il dataset da Tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitto il dataset in train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data['train'], data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomi delle classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = info.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleziono 5 classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [69, 96, 19, 16, 4]\n",
    "class_map = {69: 0, 96: 1, 19: 2, 16: 3, 4: 4} #Le mappo per usarle nella rete neurale\n",
    "print(selected_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione per filtrare le immagini che appartengono alle classi selezionate\n",
    "def filter_classes(image, label):\n",
    "    return tf.reduce_any(tf.equal(label, selected_classes)) #Restituisce True se la classe è tra quelle selezionate\n",
    "\n",
    "#Funzione per mappare le etichette originali in nuove etichette numeriche\n",
    "def map_labels(image, label):\n",
    "    label = tf.cast(label, tf.int64) #Ci assicutiamo che l'etichetta sia di tipo int64 altrimenti non e' compatibile\n",
    "\n",
    "    #Rimpiazza l'etichetta originale con una nuova etichetta numerica\n",
    "    label = tf.where(tf.equal(label, 69), tf.cast(0, tf.int64), label)\n",
    "    label = tf.where(tf.equal(label, 96), tf.cast(1, tf.int64), label)\n",
    "    label = tf.where(tf.equal(label, 19), tf.cast(2, tf.int64), label)\n",
    "    label = tf.where(tf.equal(label, 16), tf.cast(3, tf.int64), label)\n",
    "    label = tf.where(tf.equal(label, 4), tf.cast(4, tf.int64), label)\n",
    "\n",
    "    return image, label \n",
    "\n",
    "#Filtra i dati di addestramento e test in modo che contengano solo le classi selezionate\n",
    "train_data = train_data.filter(filter_classes)\n",
    "test_data = test_data.filter(filter_classes)\n",
    "\n",
    "#Applica la mappatura delle etichette ai dati di addestramento e test\n",
    "train_data = train_data.map(map_labels)\n",
    "test_data = test_data.map(map_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test se il filtro si è applicato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_data.take(10):\n",
    "    #Usiamo label_names per ottenere il nome della classe corrispondente\n",
    "    print(f\"Immagine con etichetta {label_names[label.numpy()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing delle immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione per preprocessare l'immagine e mantenerne l'etichetta\n",
    "def preprocess_image(image, label):\n",
    "    # Ridimensiona l'immagine che è la dimensione di input richiesta da ResNet50\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "\n",
    "    # Preprocessa l'immagine utilizzando la funzione di preprocessing specifica per il modello ResNet50 \n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "#Applica la funzione di preprocessamento a tutto il dataset di addestramento\n",
    "train_data = train_data.map(preprocess_image)\n",
    "\n",
    "#Applica la stessa funzione di preprocessamento a tutto il dataset di test\n",
    "test_data = test_data.map(preprocess_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch e Prefatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scegliamo la grandezza del batch size e la applichiamo al train e test\n",
    "batch_size = 32\n",
    "train_data = train_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_data = test_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello pre addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione personalizzata per calcolare precision, recall e f1-score\n",
    "def f1_metric(y_true, y_pred):\n",
    "    #Convertiamo le predizioni in one-hot encoded\n",
    "    y_pred_classes = K.argmax(y_pred, axis=-1)\n",
    "    y_true_classes = K.cast(y_true, tf.int64)\n",
    "    \n",
    "    #Precision = TP / (TP + FP)\n",
    "    true_positives = K.sum(K.cast(y_true_classes == y_pred_classes, tf.float32))\n",
    "    predicted_positives = K.sum(K.cast(y_pred_classes, tf.float32))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "    #Recall = TP / (TP + FN)\n",
    "    actual_positives = K.sum(K.cast(y_true_classes, tf.float32))\n",
    "    recall = true_positives / (actual_positives + K.epsilon())\n",
    "\n",
    "    #F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modello Pre-Addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementazione del modello resnet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congelo i pesi\n",
    "\n",
    "Per il modello fine-tunato rimuovi i commenti del for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pesi congelati di tutto il modello\n",
    "base_model.trainable = False\n",
    "\n",
    "#Per sbloccare alcuni layer finali del modello pre-addestrato\n",
    "#Questo è il modello fine-tunato\n",
    "#for layer in base_model.layers[-5:]:\n",
    "#    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il nuovo modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(), #Riduce le feature map a un vettore 1D\n",
    "    tf.keras.layers.Flatten(), #Appiattisce il vettore per i layer densi\n",
    "    tf.keras.layers.Dropout(0.5), #Previne l'overfitting disattivando il 50% dei neuroni\n",
    "    tf.keras.layers.Dense(128, activation='relu'), #Hidden layer con 128 neuroni\n",
    "    tf.keras.layers.Dense(len(selected_classes), activation='softmax') #Previsione delle classi\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), #Modifico il learning rate di Adam\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy',f1_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, validation_data=test_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricare il codice per printare i relativi grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selezionare un interprete per eseguire il codice\n",
    "#Grafico dell'andamento della Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss durante il training')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "#Grafico dell'andamento della val_accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy durante il training')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Grafico dell'andamento della Cross-Entropy Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Cross-Entropy Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Cross-Entropy Loss')\n",
    "plt.title('Cross-Entropy Loss durante il training')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzare le immagini con le rispettive predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione per visualizzare le immagini con le rispettive predizioni\n",
    "def display_predictions(model, test_data, num_images=5):\n",
    "    images, labels = next(iter(test_data))  #Prendi un batch\n",
    "    predictions = model.predict(images)  #Predici le classi per il batch di immagini\n",
    "    \n",
    "    #Visualizza le prime 'num_images' immagini insieme alle loro etichette e predizioni\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        #Trova la classe predetta\n",
    "        predicted_class = np.argmax(predictions[i])\n",
    "        \n",
    "        #Mostra la classe reale e quella predetta\n",
    "        plt.title(f\"True: {label_names[labels[i].numpy()]}\\nPred: {label_names[predicted_class]}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Chiamata della funzione per visualizzare le predizioni\n",
    "display_predictions(model, test_data, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafico per la visualizzazione della Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione per calcolare e visualizzare la Confusion Matrix\n",
    "def plot_confusion_matrix(model, test_data):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for images, labels in test_data:\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predictions = model.predict(images)\n",
    "        pred_labels.extend(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    #Calcola la Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "    #Stampa la matrice di confusione nel terminale\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "plot_confusion_matrix(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panoramica Generale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecco una panoramica delle principali fasi del processo e delle conclusioni che possiamo trarre:\n",
    "\n",
    "1. Preparazione del Dataset:\n",
    "- Abbiamo caricato il dataset Caltech-101 da TensorFlow Datasets e selezionato un sottoinsieme di 5 classi specifiche (classi 69, 96, 19, 16, 4).\n",
    "- Le immagini sono state filtrate in base a queste classi selezionate e successivamente le etichette originali sono state mappate a etichette numeriche più compatibili con la rete neurale.\n",
    "2. Preprocessing delle Immagini:\n",
    "- Le immagini sono state ridimensionate a 224x224 pixel, che è la dimensione di input richiesta dal modello ResNet50.\n",
    "- È stato applicato il preprocessing delle immagini utilizzando la funzione preprocess_input di ResNet50, che normalizza le immagini per adattarle al modello pre-addestrato.\n",
    "3. Modello di Rete Neurale:\n",
    "- Il modello utilizzato è basato su ResNet50, che è stato caricato senza la parte finale (inclusa la classificazione delle 1000 classi di ImageNet) grazie all'opzione include_top=False.\n",
    "- Il modello è stato completato con un GlobalAveragePooling2D, un Flatten, un Dropout per prevenire l'overfitting, e due layer Dense finali per la classificazione.\n",
    "- Il modello è stato compilato con l'ottimizzatore Adam e la sparse categorical cross-entropy come funzione di perdita, e come metrica l'accuratezza e il f1-score.\n",
    "4. Addestramento del Modello:\n",
    "- Il modello è stato addestrato per 5 epoche sui dati di addestramento e validato sui dati di test.\n",
    "- Durante l'addestramento, sono stati tracciati i valori di loss e accuracy, che sono stati visualizzati tramite grafici per monitorare l'andamento del modello.\n",
    "5. Valutazione del Modello:\n",
    "- I risultati dell'addestramento sono stati mostrati in grafici che illustrano l'andamento della Loss e dell'Accuracy durante le epoche, permettendo di osservare il comportamento del modello su  entrambi i set di addestramento e validazione.\n",
    "- Inoltre, il F1-score è stato calcolato per valutare il bilanciamento tra precisione e recall per ciascuna delle classi selezionate tuttavia presenta un valore insoddisfacente a causa dello sbilanciamento delle classi selezionate.\n",
    "6. Visualizzazione delle Predizioni:\n",
    "- Per meglio comprendere le prestazioni del modello, è stata creata una funzione per visualizzare alcune immagini di test con le relative etichette vere e predizioni. Questa fase permette di verificare visivamente se il modello è in grado di generalizzare correttamente su nuovi dati.\n",
    "7. Matrice di Confusione:\n",
    "- Una funzione è stata implementata per calcolare e visualizzare la matrice di confusione, che ci consente di analizzare gli errori di classificazione del modello. La matrice di confusione mostra, per ciascuna classe, quante volte il modello ha classificato correttamente o erroneamente le immagini.\n",
    "8. Modello Finetunato:\n",
    "- Infine abbiamo sbloccato i pesi degli ultimi 5 layer del modello pre-addestrato per permettere il cambio di pesi.\n",
    "- Il modello aumenta drasticamente le performance raggiungendo il picco di metriche dopo solo 3 epoche.\n",
    "\n",
    "### Conclusioni Finali\n",
    "\n",
    "Il progetto ha avuto l'obiettivo di costruire, addestrare e confrontare un modello di classificazione delle immagini basato su **ResNet50** sui dati del dataset **Caltech-101**, selezionando specifiche classi e testando due approcci: uno con il modello pre-addestrato senza fine-tuning e l'altro con fine-tuning per adattare meglio il modello ai dati specifici.\n",
    "\n",
    "#### 1. Prestazioni del Modello Pre-addestrato\n",
    "Il modello ResNet50 pre-addestrato senza fine-tuning ha mostrato una buona capacità di generalizzazione, ma le performance non sono state ottimali. La **loss** e l'**accuracy** sui dati di test hanno indicato che il modello ha beneficiato del training iniziale su ImageNet, ma non è stato completamente adattato alle specifiche 5 classi selezionate del dataset Caltech-101. Nonostante ciò, il modello pre-addestrato ha comunque ottenuto risultati di classificazione ragionevoli.\n",
    "\n",
    "#### 2. Fine-Tuning del Modello\n",
    "Quando sono stati sbloccati gli ultimi layer del modello ResNet50 e il modello è stato ri-allenato sui dati di addestramento, i risultati sono migliorati drasticamente. Il **fine-tuning** ha permesso al modello di apprendere caratteristiche più specifiche per il nostro sottoinsieme di dati. Questo approccio ha portato a un aumento significativo dell'**accuracy** e a una riduzione della **loss** durante le epoche successive. In particolare, il modello ha raggiunto un picco nelle prestazioni dopo solo 3 epoche di addestramento, suggerendo che il fine-tuning ha avuto un impatto significativo nell'adattamento del modello.\n",
    "\n",
    "#### 3. Analisi Visiva e Matrice di Confusione\n",
    "La visualizzazione delle predizioni e la matrice di confusione hanno fornito un'ulteriore comprensione delle prestazioni del modello. Le immagini di test sono state correttamente classificate per la maggior parte delle volte, con alcuni errori evidenziati dalla matrice di confusione, in particolare per le classi meno rappresentate. Tuttavia, il fine-tuning ha portato a un miglioramento nella classificazione di alcune classi che inizialmente avevano risultati più scarsi.\n",
    "\n",
    "#### 4. Confronto Pre-addestrato vs Fine-tuning\n",
    "Il confronto tra il modello pre-addestrato e quello fine-tunato ha mostrato chiaramente che il fine-tuning consente al modello di adattarsi meglio al dataset specifico. La **loss** è stata significativamente inferiore e l'**accuracy** è aumentata, soprattutto durante le epoche successive. In generale, l'approccio di fine-tuning si è rivelato fondamentale per migliorare le performance, specialmente per un dataset che non è completamente simile al dataset su cui il modello è stato originariamente addestrato (ImageNet).\n",
    "\n",
    "#### 5. Importanza del Preprocessing e delle Scelte del Modello\n",
    "Il preprocessing delle immagini (ridimensionamento e normalizzazione) è stato un passo fondamentale per preparare i dati all'ingresso nel modello ResNet50, che ha richiesto un adattamento specifico per la dimensione dell'immagine e la normalizzazione dei pixel. Inoltre, la scelta di ResNet50 come modello di base si è rivelata ottimale per questa tipologia di task, grazie alla sua architettura profonda e pre-addestramento su un ampio dataset di immagini.\n",
    "\n",
    "#### 6. Considerazioni Future\n",
    "Nonostante i buoni risultati ottenuti, potrebbero essere esplorate ulteriori ottimizzazioni, come l'uso di tecniche avanzate di data augmentation, la sblocco di più strati per un fine-tuning più profondo, o l'uso di modelli più leggeri o specializzati per task di classificazione di immagini più mirati. Inoltre, l'analisi delle metriche come il **F1-score** e l'**accuracy** per ciascuna classe, potrebbe essere utile per capire meglio le aree in cui il modello ha bisogno di miglioramenti.\n",
    "\n",
    "In conclusione, il progetto ha dimostrato che l'uso di modelli pre-addestrati come **ResNet50**, combinato con il fine-tuning, è un approccio potente ed efficace per la classificazione delle immagini, consentendo di ottenere ottime performance anche su un dataset specifico come Caltech-101. Il fine-tuning in particolare è essenziale per adattare modelli generali ai dati di dominio specifico, portando a miglioramenti significativi nelle prestazioni complessive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predizioni del modello Pre-addestrato**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Epoch 1/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 26s 5s/step - accuracy: 0.2604 - f1_metric: 0.1525 - loss: 1.8036 - val_accuracy: 0.6874 - val_f1_metric: 0.1942 - val_loss: 0.7745\n",
    "\n",
    "Epoch 2/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 22s 5s/step - accuracy: 0.8048 - f1_metric: 0.4020 - loss: 0.4866 - val_accuracy: 0.8497 - val_f1_metric: 0.2336 - val_loss: 0.4185\n",
    "\n",
    "Epoch 3/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 21s 5s/step - accuracy: 0.9574 - f1_metric: 0.4963 - loss: 0.1474 - val_accuracy: 0.8877 - val_f1_metric: 0.2429 - val_loss: 0.3175\n",
    "\n",
    "Epoch 4/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 21s 5s/step - accuracy: 0.9871 - f1_metric: 0.5100 - loss: 0.0712 - val_accuracy: 0.8722 - val_f1_metric: 0.2413 - val_loss: 0.3138\n",
    "\n",
    "Epoch 5/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 21s 5s/step - accuracy: 0.9833 - f1_metric: 0.5144 - loss: 0.0325 - val_accuracy: 0.9171 - val_f1_metric: 0.2474 - val_loss: 0.2231\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "[[  9   0   0   0   0]\n",
    " [  0   7   0   0   0]\n",
    " [  0   0  31   0   1]\n",
    " [  0   0   0  93   0]\n",
    " [ 18   9  15   6 390]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predizioni del modello Fine-tunato**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Epoch 1/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 26s 5s/step - accuracy: 0.3420 - f1_metric: 0.1877 - loss: 1.8527 - val_accuracy: 0.8463 - val_f1_metric: 0.2268 - val_loss: 0.3832\n",
    "\n",
    "Epoch 2/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 22s 5s/step - accuracy: 0.9850 - f1_metric: 0.5042 - loss: 0.1198 - val_accuracy: 0.8377 - val_f1_metric: 0.2297 - val_loss: 0.5252\n",
    "\n",
    "Epoch 3/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 22s 5s/step - accuracy: 0.9921 - f1_metric: 0.5088 - loss: 0.0273 - val_accuracy: 0.8808 - val_f1_metric: 0.2402 - val_loss: 0.4488\n",
    "\n",
    "Epoch 4/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 22s 5s/step - accuracy: 1.0000 - f1_metric: 0.5165 - loss: 0.0154 - val_accuracy: 0.9102 - val_f1_metric: 0.2455 - val_loss: 0.3583\n",
    "\n",
    "Epoch 5/5\n",
    "5/5 ━━━━━━━━━━━━━━━━━━━━ 22s 5s/step - accuracy: 1.0000 - f1_metric: 0.5165 - loss: 0.0045 - val_accuracy: 0.9361 - val_f1_metric: 0.2493 - val_loss: 0.2034\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "[[  9   0   0   0   0]\n",
    " [  0   7   0   0   0]\n",
    " [  0   0  28   0   4]\n",
    " [  0   0   0  93   0]\n",
    " [  7  10   9   2 410]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
